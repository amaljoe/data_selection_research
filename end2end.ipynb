{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T14:29:20.257465Z",
     "start_time": "2025-03-24T14:29:18.732122Z"
    }
   },
   "source": [
    "from data_loader import get_mix_instruct\n",
    "\n",
    "prompts, references, ds_name = get_mix_instruct(\"train\", 1000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mix-instruct_train_1000 found in cache, loading from cache ✅\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:29:25.755334Z",
     "start_time": "2025-03-24T14:29:20.284358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utility_functions.delift_se import get_delift_se_utility\n",
    "\n",
    "utility, utility_name = get_delift_se_utility(prompts, references, ds_name)"
   ],
   "id": "972fcd9248770df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility: mix-instruct_train_1000_delift-se found in cache, loading from cache ✅\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:29:26.372029Z",
     "start_time": "2025-03-24T14:29:25.965548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from subset import create_subset\n",
    "\n",
    "subset, subset_name = create_subset(utility, utility_name)"
   ],
   "id": "5b8c7f2b2b19e427",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: mix-instruct_train_1000_delift-se_0.3 found in cache, loading from cache ✅\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:29:32.819878Z",
     "start_time": "2025-03-24T14:29:26.387701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, attn_implementation=\"flash_attention_2\",torch_dtype=torch.bfloat16).to(device)"
   ],
   "id": "ee66d80efd61a974",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99fbd9bfb0d94d39ab5f8066c0241f47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:29:33.723558Z",
     "start_time": "2025-03-24T14:29:32.842098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_responses(prompts, tokenizer, model, batch_size=8, max_length=512):\n",
    "    dataloader = DataLoader(prompts, batch_size=batch_size, shuffle=False)\n",
    "    all_responses = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Generating responses\"):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_length, do_sample=False)\n",
    "        responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        all_responses.extend(responses)\n",
    "\n",
    "    return all_responses\n",
    "\n",
    "\n",
    "generate_responses(prompts[:100], tokenizer, model)"
   ],
   "id": "eb596f8033c54443",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/13 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "Generating responses:   0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'get_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     13\u001B[39m         all_responses.extend(responses)\n\u001B[32m     15\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m all_responses\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43mgenerate_responses\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mgenerate_responses\u001B[39m\u001B[34m(prompts, tokenizer, model, batch_size, max_length)\u001B[39m\n\u001B[32m      9\u001B[39m inputs = tokenizer(batch, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m, padding=\u001B[38;5;28;01mTrue\u001B[39;00m, truncation=\u001B[38;5;28;01mTrue\u001B[39;00m).to(device)\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m responses = [tokenizer.decode(output, skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[32m     13\u001B[39m all_responses.extend(responses)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/transformers/generation/utils.py:2326\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[39m\n\u001B[32m   2318\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2319\u001B[39m         input_ids=input_ids,\n\u001B[32m   2320\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2321\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2322\u001B[39m         **model_kwargs,\n\u001B[32m   2323\u001B[39m     )\n\u001B[32m   2325\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2326\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2331\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2333\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2334\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2336\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2337\u001B[39m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[32m   2338\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2339\u001B[39m         input_ids=input_ids,\n\u001B[32m   2340\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2341\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2342\u001B[39m         **model_kwargs,\n\u001B[32m   2343\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/transformers/generation/utils.py:3279\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3276\u001B[39m is_prefill = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3277\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001B[32m   3278\u001B[39m     \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3279\u001B[39m     model_inputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprepare_inputs_for_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3281\u001B[39m     \u001B[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001B[39;00m\n\u001B[32m   3282\u001B[39m     model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_attentions\u001B[39m\u001B[33m\"\u001B[39m: output_attentions} \u001B[38;5;28;01mif\u001B[39;00m output_attentions \u001B[38;5;28;01melse\u001B[39;00m {})\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1292\u001B[39m, in \u001B[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001B[39m\u001B[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001B[39m\n\u001B[32m   1290\u001B[39m     cache_length = past_key_values.get_seq_length()\n\u001B[32m   1291\u001B[39m     past_length = past_key_values.seen_tokens\n\u001B[32m-> \u001B[39m\u001B[32m1292\u001B[39m     max_cache_length = \u001B[43mpast_key_values\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_max_length\u001B[49m()\n\u001B[32m   1293\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1294\u001B[39m     cache_length = past_length = past_key_values[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m].shape[\u001B[32m2\u001B[39m]\n",
      "\u001B[31mAttributeError\u001B[39m: 'DynamicCache' object has no attribute 'get_max_length'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:32:08.488794Z",
     "start_time": "2025-03-24T14:32:08.263877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = DataLoader(prompts, batch_size=8, shuffle=False)\n",
    "batch = next(iter(dataloader))\n",
    "inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")"
   ],
   "id": "bad8adb98a2cff4e",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'get_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m batch = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(dataloader))\n\u001B[32m      3\u001B[39m inputs = tokenizer(batch, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m, padding=\u001B[38;5;28;01mTrue\u001B[39;00m, truncation=\u001B[38;5;28;01mTrue\u001B[39;00m).to(device)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcache_position\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/transformers/generation/utils.py:2326\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[39m\n\u001B[32m   2318\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2319\u001B[39m         input_ids=input_ids,\n\u001B[32m   2320\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2321\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2322\u001B[39m         **model_kwargs,\n\u001B[32m   2323\u001B[39m     )\n\u001B[32m   2325\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2326\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2331\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2333\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2334\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2336\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2337\u001B[39m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[32m   2338\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2339\u001B[39m         input_ids=input_ids,\n\u001B[32m   2340\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2341\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2342\u001B[39m         **model_kwargs,\n\u001B[32m   2343\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/llm-general2/lib/python3.12/site-packages/transformers/generation/utils.py:3279\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3276\u001B[39m is_prefill = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3277\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001B[32m   3278\u001B[39m     \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3279\u001B[39m     model_inputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprepare_inputs_for_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3281\u001B[39m     \u001B[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001B[39;00m\n\u001B[32m   3282\u001B[39m     model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_attentions\u001B[39m\u001B[33m\"\u001B[39m: output_attentions} \u001B[38;5;28;01mif\u001B[39;00m output_attentions \u001B[38;5;28;01melse\u001B[39;00m {})\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1292\u001B[39m, in \u001B[36mPhi3ForCausalLM.prepare_inputs_for_generation\u001B[39m\u001B[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, **kwargs)\u001B[39m\n\u001B[32m   1290\u001B[39m     cache_length = past_key_values.get_seq_length()\n\u001B[32m   1291\u001B[39m     past_length = past_key_values.seen_tokens\n\u001B[32m-> \u001B[39m\u001B[32m1292\u001B[39m     max_cache_length = \u001B[43mpast_key_values\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_max_length\u001B[49m()\n\u001B[32m   1293\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1294\u001B[39m     cache_length = past_length = past_key_values[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m].shape[\u001B[32m2\u001B[39m]\n",
      "\u001B[31mAttributeError\u001B[39m: 'DynamicCache' object has no attribute 'get_max_length'"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
